[TOC]



## Ex1

### Mse / 2 ，归一化

```
Train Loss:
 [0.09566953, 0.010694978, 0.004365819, 0.0018175665, 0.0007625969, 0.00032301687, 0.00013708518, 6.0287868e-05, 2.7964898e-05, 1.2988224e-05, 7.280114e-06, 5.8223914e-06, 5.8223914e-06, 5.8223914e-06, 5.8223914e-06, 5.8223914e-06]

Test Loss:
 [0.122102775, 0.0068427073, 0.0024256264, 0.0008579258, 0.0002942976, 0.000104038816, 5.0109502e-05, 4.431181e-05, 5.1905798e-05, 6.3162064e-05, 7.358234e-05, 7.497685e-05, 7.497685e-05, 7.497685e-05, 7.497685e-05, 7.497685e-05]
 
iteration times: 0
W: [[2.5816895e-05]
 [1.6823726e-05]] 
b: [5.6524175e-05] 
Train Loss: 0.09566953
Test Loss: 0.122102775

iteration times: 100000
W: [[ 0.29399818]
 [-0.34172985]] 
b: [0.38485304] 
Train Loss: 0.010694978
Test Loss: 0.0068427073

iteration times: 200000
W: [[ 0.44556725]
 [-0.60313416]] 
b: [0.42715162] 
Train Loss: 0.004365819
Test Loss: 0.0024256264

iteration times: 300000
W: [[ 0.557119 ]
 [-0.7607374]] 
b: [0.44373533] 
Train Loss: 0.0018175665
Test Loss: 0.0008579258

iteration times: 400000
W: [[ 0.6338206]
 [-0.8587512]] 
b: [0.4508256] 
Train Loss: 0.0007625969
Test Loss: 0.0002942976

iteration times: 500000
W: [[ 0.68474984]
 [-0.92067146]] 
b: [0.4542353] 
Train Loss: 0.00032301687
Test Loss: 0.000104038816

iteration times: 600000
W: [[ 0.71869886]
 [-0.96047455]] 
b: [0.4558635] 
Train Loss: 0.00013708518
Test Loss: 5.0109502e-05

iteration times: 700000
W: [[ 0.7401436 ]
 [-0.98584706]] 
b: [0.45692566] 
Train Loss: 6.0287868e-05
Test Loss: 4.431181e-05

iteration times: 800000
W: [[ 0.75430346]
 [-1.0017053 ]] 
b: [0.45734078] 
Train Loss: 2.7964898e-05
Test Loss: 5.1905798e-05

iteration times: 900000
W: [[ 0.76361966]
 [-1.0136262 ]] 
b: [0.4580757] 
Train Loss: 1.2988224e-05
Test Loss: 6.3162064e-05

iteration times: 1000000
W: [[ 0.7695801]
 [-1.0201465]] 
b: [0.458368] 
Train Loss: 7.280114e-06
Test Loss: 7.358234e-05

iteration times: 1100000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1200000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1300000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1400000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1500000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05
```



### Mse / 2 

```
Train Loss:
 [49610.42, 33.503178, 7.535531, 2.9105134, 2.089079, 1.9431273, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444]

Test Loss:
 [13945.736, 62.241535, 59.77898, 63.006298, 65.11604, 66.11772, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166]
 
iteration times: 100000
W: [[  7.089414]
 [-72.76044 ]] 
b: [46.329998] 
Train Loss: 33.503178
Test Loss: 62.241535

iteration times: 200000
W: [[  6.9002647]
 [-72.54343  ]] 
b: [65.48001] 
Train Loss: 7.535531
Test Loss: 59.77898

iteration times: 300000
W: [[  6.8204446]
 [-72.45184  ]] 
b: [73.5612] 
Train Loss: 2.9105134
Test Loss: 63.006298

iteration times: 400000
W: [[  6.7869325]
 [-72.41338  ]] 
b: [76.95402] 
Train Loss: 2.089079
Test Loss: 65.11604

iteration times: 500000
W: [[  6.773085]
 [-72.3975  ]] 
b: [78.355934] 
Train Loss: 1.9431273
Test Loss: 66.11772

iteration times: 600000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 700000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 800000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 900000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1000000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1100000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1200000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1300000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1400000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1500000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166
```



### Mse / 2 标准化

```
Train Loss:
 [0.49992374, 0.11783889, 0.02875953, 0.007059803, 0.0017638403, 0.0004712305, 0.0001557598, 7.8765006e-05, 6.0042352e-05, 5.5465913e-05, 5.4192533e-05, 5.3944546e-05, 5.3944546e-05, 5.3944543e-05, 5.3944543e-05, 5.3944543e-05]

Test Loss:
 [0.32022828, 0.06420137, 0.012665257, 0.002414167, 0.0009774234, 0.0011508439, 0.0014518047, 0.0016526354, 0.0017628003, 0.0018181395, 0.0018534504, 0.0018593727, 0.0018593728, 0.0018593728, 0.0018593731, 0.0018593731]
 
iteration times: 0
W: [[ 4.6462468e-05]
 [-9.6321557e-05]] 
b: [2.2351743e-12] 
Train Loss: 0.49992374
Test Loss: 0.32022828

iteration times: 10000
W: [[ 0.41462547]
 [-0.61000365]] 
b: [1.0365532e-09] 
Train Loss: 0.11783889
Test Loss: 0.06420137

iteration times: 20000
W: [[ 0.6578549]
 [-0.8729121]] 
b: [-1.8338183e-09] 
Train Loss: 0.02875953
Test Loss: 0.012665257

iteration times: 30000
W: [[ 0.7818897]
 [-0.9989292]] 
b: [-6.1775296e-09] 
Train Loss: 0.007059803
Test Loss: 0.002414167

iteration times: 40000
W: [[ 0.84355754]
 [-1.0607957 ]] 
b: [-8.690617e-09] 
Train Loss: 0.0017638403
Test Loss: 0.0009774234

iteration times: 50000
W: [[ 0.8740607]
 [-1.0913202]] 
b: [-1.10106395e-08] 
Train Loss: 0.0004712305
Test Loss: 0.0011508439

iteration times: 60000
W: [[ 0.88913333]
 [-1.1063876 ]] 
b: [-1.29861295e-08] 
Train Loss: 0.0001557598
Test Loss: 0.0014518047

iteration times: 70000
W: [[ 0.896575 ]
 [-1.1138185]] 
b: [-1.4902232e-08] 
Train Loss: 7.8765006e-05
Test Loss: 0.0016526354

iteration times: 80000
W: [[ 0.90022725]
 [-1.1174389 ]] 
b: [-1.4895818e-08] 
Train Loss: 6.0042352e-05
Test Loss: 0.0017628003

iteration times: 90000
W: [[ 0.90199596]
 [-1.1191686 ]] 
b: [-1.489953e-08] 
Train Loss: 5.5465913e-05
Test Loss: 0.0018181395

iteration times: 100000
W: [[ 0.9028544]
 [-1.1201917]] 
b: [-1.48920325e-08] 
Train Loss: 5.4192533e-05
Test Loss: 0.0018534504

iteration times: 110000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [-1.0308481e-08] 
Train Loss: 5.3944546e-05
Test Loss: 0.0018593727

iteration times: 120000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.9514282e-09] 
Train Loss: 5.3944546e-05
Test Loss: 0.0018593728

iteration times: 130000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [1.321041e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593728

iteration times: 140000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [1.6832825e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731

iteration times: 150000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [1.8881746e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731
```



### Sum 归一化

```
Train Loss:
 [9.29617, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752]

Test Loss:
 [2.3778727, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807]
 
iteration times: 0
W: [[0.00258169]
 [0.00168237]] 
b: [0.00565242] 
Train Loss: 9.29617
Test Loss: 2.3778727

iteration times: 100000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 200000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 300000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 400000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 500000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 600000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 700000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 800000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 900000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1000000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1100000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1200000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1300000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1400000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1500000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807
```





## Ex2

### Mse / 2

```
Train Loss:
 [129877.92, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]

Test Loss:
 [58602.49, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
 
 iteration times: 0
W: [[8.409362 ]
 [0.3310432]] 
b: [0.08022387] 
Train Loss: 129877.92
Test Loss: 58602.49

iteration times: 100000
W: [[nan]
 [nan]] 
b: [nan] 
Train Loss: nan
Test Loss: nan

```



### Mse / 2 归一化

```
Train Loss:
 [0.095660314, 0.007903346, 0.0024309573, 0.0007622265, 0.00024147338, 7.796503e-05, 2.6596446e-05, 1.1197945e-05, 5.463813e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06]

Test Loss:
 [0.122091904, 0.004828136, 0.0012166699, 0.00029435699, 7.637401e-05, 4.368423e-05, 5.2485626e-05, 6.337282e-05, 7.764882e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05]
 
iteration times: 0
W: [[3.4422526e-05]
 [2.2431634e-05]] 
b: [7.536556e-05] 
Train Loss: 0.095660314
Test Loss: 0.122091904

iteration times: 100000
W: [[ 0.34876078]
 [-0.4445611 ]] 
b: [0.40364408] 
Train Loss: 0.007903346
Test Loss: 0.004828136

iteration times: 200000
W: [[ 0.52424943]
 [-0.71647364]] 
b: [0.43973732] 
Train Loss: 0.0024309573
Test Loss: 0.0012166699

iteration times: 300000
W: [[ 0.63378  ]
 [-0.8588557]] 
b: [0.45082736] 
Train Loss: 0.0007622265
Test Loss: 0.00029435699

iteration times: 400000
W: [[ 0.69782954]
 [-0.93618083]] 
b: [0.45485282] 
Train Loss: 0.00024147338
Test Loss: 7.637401e-05

iteration times: 500000
W: [[ 0.7344782]
 [-0.9788271]] 
b: [0.45655513] 
Train Loss: 7.796503e-05
Test Loss: 4.368423e-05

iteration times: 600000
W: [[ 0.755049 ]
 [-1.0025738]] 
b: [0.4573638] 
Train Loss: 2.6596446e-05
Test Loss: 5.2485626e-05

iteration times: 700000
W: [[ 0.76619464]
 [-1.0145932 ]] 
b: [0.45742148] 
Train Loss: 1.1197945e-05
Test Loss: 6.337282e-05

iteration times: 800000
W: [[ 0.7721551]
 [-1.0231423]] 
b: [0.45845333] 
Train Loss: 5.463813e-06
Test Loss: 7.764882e-05

iteration times: 900000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1000000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1100000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1200000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1300000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1400000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1500000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05
```



### Mse / 2 标准化

```
Train Loss:
 [0.49989837, 0.073583096, 0.011263316, 0.0017635908, 0.00031460446, 9.361457e-05, 5.991997e-05, 5.4806747e-05, 5.394061e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05]

Test Loss:
 [0.32021007, 0.03774156, 0.004121556, 0.00097744, 0.0012583665, 0.0015977271, 0.0017645427, 0.0018322133, 0.0018626321, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311]

iteration times: 0
W: [[ 6.1949955e-05]
 [-1.2842873e-04]] 
b: [2.9802322e-12] 
Train Loss: 0.49989837
Test Loss: 0.32021007

iteration times: 10000
W: [[ 0.5135576 ]
 [-0.72063833]] 
b: [3.0100672e-10] 
Train Loss: 0.073583096
Test Loss: 0.03774156

iteration times: 20000
W: [[ 0.7496775]
 [-0.9664619]] 
b: [-4.9434368e-09] 
Train Loss: 0.011263316
Test Loss: 0.004121556

iteration times: 30000
W: [[ 0.8435611]
 [-1.0608009]] 
b: [-8.734851e-09] 
Train Loss: 0.0017635908
Test Loss: 0.00097744

iteration times: 40000
W: [[ 0.8803038]
 [-1.097566 ]] 
b: [-1.14694965e-08] 
Train Loss: 0.00031460446
Test Loss: 0.0012583665

iteration times: 50000
W: [[ 0.89465755]
 [-1.1119078 ]] 
b: [-1.4313571e-08] 
Train Loss: 9.361457e-05
Test Loss: 0.0015977271

iteration times: 60000
W: [[ 0.90025073]
 [-1.1174866 ]] 
b: [-1.4891524e-08] 
Train Loss: 5.991997e-05
Test Loss: 0.0017645427

iteration times: 70000
W: [[ 0.9023956]
 [-1.1195905]] 
b: [-1.4896083e-08] 
Train Loss: 5.4806747e-05
Test Loss: 0.0018322133

iteration times: 80000
W: [[ 0.9032218]
 [-1.1204858]] 
b: [-1.4898565e-08] 
Train Loss: 5.394061e-05
Test Loss: 0.0018626321

iteration times: 90000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.5106952e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

iteration times: 100000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.5426696e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

iteration times: 110000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.574644e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

iteration times: 120000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.6066185e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

iteration times: 130000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.6385929e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

iteration times: 140000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.6705673e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

iteration times: 150000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.7025418e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

```



## Ex3

### Mse / 2

```
Train Loss:
 [88862.46, 161.6851, 127.918045, 108.15653, 92.329544, 78.49047, 66.81178, 60.5053, 46.82453, 43.613724, 36.005184, 30.851025, 25.451572, 24.91912, 17.78771, 15.59241]

Test Loss:
 [113332.516, 144.4163, 98.57767, 91.89929, 94.36559, 66.74569, 61.61593, 98.280365, 63.856804, 44.74122, 82.44847, 44.956745, 73.33224, 87.98949, 60.099987, 67.05955]

iteration times: 0
W: [[0.22820099]
 [0.01245753]] 
b: [0.003741] 
Train Loss: 88862.46
Test Loss: 113332.516

iteration times: 100000
W: [[  7.5171194]
 [-73.1933   ]] 
b: [7.3581514] 
Train Loss: 161.6851
Test Loss: 144.4163

iteration times: 200000
W: [[  7.4131837]
 [-73.12359  ]] 
b: [13.298775] 
Train Loss: 127.918045
Test Loss: 98.57767

iteration times: 300000
W: [[  7.365377]
 [-73.12874 ]] 
b: [18.706495] 
Train Loss: 108.15653
Test Loss: 91.89929

iteration times: 400000
W: [[  7.328045]
 [-73.05038 ]] 
b: [23.729918] 
Train Loss: 92.329544
Test Loss: 94.36559

iteration times: 500000
W: [[  7.249613]
 [-72.93348 ]] 
b: [28.344261] 
Train Loss: 78.49047
Test Loss: 66.74569

iteration times: 600000
W: [[  7.209701]
 [-72.94943 ]] 
b: [32.57215] 
Train Loss: 66.81178
Test Loss: 61.61593

iteration times: 700000
W: [[  7.2189336]
 [-72.88338  ]] 
b: [36.447006] 
Train Loss: 60.5053
Test Loss: 98.280365

iteration times: 800000
W: [[  7.1508927]
 [-72.9032   ]] 
b: [40.013485] 
Train Loss: 46.82453
Test Loss: 63.856804

iteration times: 900000
W: [[  7.092128]
 [-72.794174]] 
b: [43.28909] 
Train Loss: 43.613724
Test Loss: 44.74122

iteration times: 1000000
W: [[  7.1114907]
 [-72.77192  ]] 
b: [46.285652] 
Train Loss: 36.005184
Test Loss: 82.44847

iteration times: 1100000
W: [[  7.0416465]
 [-72.72149  ]] 
b: [49.039402] 
Train Loss: 30.851025
Test Loss: 44.956745

iteration times: 1200000
W: [[  7.0510216]
 [-72.677795 ]] 
b: [51.55058] 
Train Loss: 25.451572
Test Loss: 73.33224

iteration times: 1300000
W: [[  7.04244]
 [-72.67177]] 
b: [53.871117] 
Train Loss: 24.91912
Test Loss: 87.98949

iteration times: 1400000
W: [[  6.9947233]
 [-72.63975  ]] 
b: [55.979557] 
Train Loss: 17.78771
Test Loss: 60.099987

iteration times: 1500000
W: [[  6.983691]
 [-72.64849 ]] 
b: [57.92826] 
Train Loss: 15.59241
Test Loss: 67.05955
```



### Mse / 2 归一化

```
Train Loss:
 [0.09569326, 0.026291719, 0.02281258, 0.020685606, 0.018772917, 0.017057486, 0.015512738, 0.014111637, 0.012857355, 0.011722922, 0.010690902, 0.009750936, 0.008906187, 0.008138204, 0.0074382448, 0.0067942357]

Test Loss:
 [0.12213083, 0.02453852, 0.016967352, 0.014727803, 0.013070367, 0.011766525, 0.01053579, 0.009377027, 0.008413457, 0.0076271156, 0.006848436, 0.0061822557, 0.00556877, 0.005002712, 0.004475654, 0.004023999]
 
iteration times: 0
W: [[3.7903471e-06]
 [2.3252617e-06]] 
b: [7.903277e-06] 
Train Loss: 0.09569326
Test Loss: 0.12213083

iteration times: 100000
W: [[0.12153976]
 [0.03973099]] 
b: [0.25352317] 
Train Loss: 0.026291719
Test Loss: 0.02453852

iteration times: 200000
W: [[ 0.1521076 ]
 [-0.00208369]] 
b: [0.29956076] 
Train Loss: 0.02281258
Test Loss: 0.016967352

iteration times: 300000
W: [[ 0.17105478]
 [-0.05198689]] 
b: [0.3168777] 
Train Loss: 0.020685606
Test Loss: 0.014727803

iteration times: 400000
W: [[ 0.18912227]
 [-0.1005742 ]] 
b: [0.33042502] 
Train Loss: 0.018772917
Test Loss: 0.013070367

iteration times: 500000
W: [[ 0.2063749 ]
 [-0.14699244]] 
b: [0.34146872] 
Train Loss: 0.017057486
Test Loss: 0.011766525

iteration times: 600000
W: [[ 0.22443287]
 [-0.19040132]] 
b: [0.35187012] 
Train Loss: 0.015512738
Test Loss: 0.01053579

iteration times: 700000
W: [[ 0.24254754]
 [-0.2314615 ]] 
b: [0.36203918] 
Train Loss: 0.014111637
Test Loss: 0.009377027

iteration times: 800000
W: [[ 0.2599809 ]
 [-0.27020526]] 
b: [0.37066454] 
Train Loss: 0.012857355
Test Loss: 0.008413457

iteration times: 900000
W: [[ 0.2769603 ]
 [-0.30712897]] 
b: [0.3775903] 
Train Loss: 0.011722922
Test Loss: 0.0076271156

iteration times: 1000000
W: [[ 0.29399535]
 [-0.34192628]] 
b: [0.38478577] 
Train Loss: 0.010690902
Test Loss: 0.006848436

iteration times: 1100000
W: [[ 0.31059495]
 [-0.37522328]] 
b: [0.39093933] 
Train Loss: 0.009750936
Test Loss: 0.0061822557

iteration times: 1200000
W: [[ 0.32703787]
 [-0.40623444]] 
b: [0.3966607] 
Train Loss: 0.008906187
Test Loss: 0.00556877

iteration times: 1300000
W: [[ 0.34329104]
 [-0.43549478]] 
b: [0.40206233] 
Train Loss: 0.008138204
Test Loss: 0.005002712

iteration times: 1400000
W: [[ 0.35926524]
 [-0.46319887]] 
b: [0.40733308] 
Train Loss: 0.0074382448
Test Loss: 0.004475654

iteration times: 1500000
W: [[ 0.37492728]
 [-0.48988345]] 
b: [0.41171724] 
Train Loss: 0.0067942357
Test Loss: 0.004023999
```



### Mse / 2 标准化 random

```
Train Loss:
 [0.49978846, 5.417346e-05, 5.3752385e-05, 5.3750882e-05, 5.37738e-05, 5.3756507e-05, 5.375594e-05, 5.3757325e-05, 5.3751264e-05, 5.3757838e-05, 5.3769953e-05, 5.3750828e-05, 5.3765536e-05, 5.375574e-05, 5.3752585e-05, 5.3754262e-05]

Test Loss:
 [0.32003197, 0.0018539574, 0.0018885664, 0.0018839039, 0.0018842658, 0.0018811045, 0.0018927259, 0.0018905106, 0.0018855126, 0.0018821743, 0.0018811247, 0.0018854232, 0.0018805597, 0.0018882852, 0.0018781315, 0.0018771279]

iteration times: 0
W: [[ 0.00019131]
 [-0.00023719]] 
b: [0.00025572] 
Train Loss: 0.49978846
Test Loss: 0.32003197

iteration times: 100000
W: [[ 0.9029002]
 [-1.1201841]] 
b: [2.120025e-05] 
Train Loss: 5.417346e-05
Test Loss: 0.0018539574

iteration times: 200000
W: [[ 0.9038245]
 [-1.1211646]] 
b: [3.9725852e-05] 
Train Loss: 5.3752385e-05
Test Loss: 0.0018885664

iteration times: 300000
W: [[ 0.9038027]
 [-1.1210732]] 
b: [2.3585666e-05] 
Train Loss: 5.3750882e-05
Test Loss: 0.0018839039

iteration times: 400000
W: [[ 0.903907 ]
 [-1.1213198]] 
b: [-0.00013346] 
Train Loss: 5.37738e-05
Test Loss: 0.0018842658

iteration times: 500000
W: [[ 0.90388054]
 [-1.1211959 ]] 
b: [-0.00010121] 
Train Loss: 5.3756507e-05
Test Loss: 0.0018811045

iteration times: 600000
W: [[ 0.9038876]
 [-1.1212163]] 
b: [8.202683e-05] 
Train Loss: 5.375594e-05
Test Loss: 0.0018927259

iteration times: 700000
W: [[ 0.9039709]
 [-1.1211758]] 
b: [8.628383e-05] 
Train Loss: 5.3757325e-05
Test Loss: 0.0018905106

iteration times: 800000
W: [[ 0.9038429]
 [-1.1210796]] 
b: [5.2293755e-05] 
Train Loss: 5.3751264e-05
Test Loss: 0.0018855126

iteration times: 900000
W: [[ 0.90390235]
 [-1.1210254 ]] 
b: [4.3713357e-05] 
Train Loss: 5.3757838e-05
Test Loss: 0.0018821743

iteration times: 1000000
W: [[ 0.904035 ]
 [-1.1210749]] 
b: [9.630537e-06] 
Train Loss: 5.3769953e-05
Test Loss: 0.0018811247

iteration times: 1100000
W: [[ 0.90384877]
 [-1.1210866 ]] 
b: [4.655422e-05] 
Train Loss: 5.3750828e-05
Test Loss: 0.0018854232

iteration times: 1200000
W: [[ 0.90380436]
 [-1.121208  ]] 
b: [-0.00013054] 
Train Loss: 5.3765536e-05
Test Loss: 0.0018805597

iteration times: 1300000
W: [[ 0.9039894]
 [-1.1212033]] 
b: [3.1375257e-05] 
Train Loss: 5.375574e-05
Test Loss: 0.0018882852

iteration times: 1400000
W: [[ 0.90385807]
 [-1.1210625 ]] 
b: [-5.8330625e-05] 
Train Loss: 5.3752585e-05
Test Loss: 0.0018781315

iteration times: 1500000
W: [[ 0.9038707]
 [-1.1210833]] 
b: [-8.8550645e-05] 
Train Loss: 5.3754262e-05
Test Loss: 0.0018771279
```



### Mse / 2 标准化 mini batch

```
Train Loss:
 [0.49986112, 5.4115448e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05, 5.3752112e-05]

Test Loss:
 [0.3200914, 0.00185396, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683, 0.0018809683]

iteration times: 0
W: [[-1.2158955e-06]
 [-2.1684990e-04]] 
b: [0.00019137] 
Train Loss: 0.49986112
Test Loss: 0.3200914

iteration times: 100000
W: [[ 0.9029868]
 [-1.1202316]] 
b: [1.5226635e-07] 
Train Loss: 5.4115448e-05
Test Loss: 0.00185396

iteration times: 200000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555532e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 300000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555591e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 400000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.5555615e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 500000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555532e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 600000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555591e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 700000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.5555615e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 800000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555532e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 900000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555591e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 1000000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.5555615e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 1100000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555532e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 1200000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555591e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 1300000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.5555615e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 1400000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555532e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683

iteration times: 1500000
W: [[ 0.90379936]
 [-1.1210359 ]] 
b: [3.555591e-07] 
Train Loss: 5.3752112e-05
Test Loss: 0.0018809683
```



### Mse / 2 random

```
Train Loss:
 [20466.457, 42.465607, 7.410732, 49.053585, 7.9310045, 2.7603793, 4.0986266, 10.21915, 5.302797, 23.942385, 1.9983852, 2.9849548, 3.3160794, 338.99677, 37.093956, 7.6929936]

Test Loss:
 [24835.1, 123.35355, 74.88098, 6.7383485, 36.581615, 75.55799, 88.85445, 112.9167, 91.68703, 17.425694, 72.87447, 54.223255, 89.71957, 612.89966, 11.381426, 37.497906]

iteration times: 0
W: [[3.0539942]
 [0.2666868]] 
b: [0.021507] 
Train Loss: 20466.457
Test Loss: 24835.1

iteration times: 100000
W: [[  7.1509385]
 [-73.355316 ]] 
b: [47.55799] 
Train Loss: 42.465607
Test Loss: 123.35355

iteration times: 200000
W: [[  6.9136963]
 [-72.83312  ]] 
b: [66.44121] 
Train Loss: 7.410732
Test Loss: 74.88098

iteration times: 300000
W: [[  6.728952]
 [-72.558266]] 
b: [74.05151] 
Train Loss: 49.053585
Test Loss: 6.7383485

iteration times: 400000
W: [[  6.75805]
 [-72.4958 ]] 
b: [76.98437] 
Train Loss: 7.9310045
Test Loss: 36.581615

iteration times: 500000
W: [[  6.776351]
 [-72.23596 ]] 
b: [78.39026] 
Train Loss: 2.7603793
Test Loss: 75.55799

iteration times: 600000
W: [[  6.783423]
 [-72.338394]] 
b: [79.078384] 
Train Loss: 4.0986266
Test Loss: 88.85445

iteration times: 700000
W: [[  6.7939296]
 [-72.26272  ]] 
b: [79.58068] 
Train Loss: 10.21915
Test Loss: 112.9167

iteration times: 800000
W: [[  6.777287]
 [-72.1997  ]] 
b: [79.48288] 
Train Loss: 5.302797
Test Loss: 91.68703

iteration times: 900000
W: [[  6.701753]
 [-72.45067 ]] 
b: [79.467] 
Train Loss: 23.942385
Test Loss: 17.425694

iteration times: 1000000
W: [[  6.7700686]
 [-72.497505 ]] 
b: [79.53738] 
Train Loss: 1.9983852
Test Loss: 72.87447

iteration times: 1100000
W: [[  6.752708]
 [-72.46115 ]] 
b: [79.38956] 
Train Loss: 2.9849548
Test Loss: 54.223255

iteration times: 1200000
W: [[  6.788285]
 [-72.591324]] 
b: [79.41145] 
Train Loss: 3.3160794
Test Loss: 89.71957

iteration times: 1300000
W: [[  7.002777]
 [-72.245766]] 
b: [79.617455] 
Train Loss: 338.99677
Test Loss: 612.89966

iteration times: 1400000
W: [[  6.6909904]
 [-72.56575  ]] 
b: [79.45059] 
Train Loss: 37.093956
Test Loss: 11.381426

iteration times: 1500000
W: [[  6.731775]
 [-72.43853 ]] 
b: [79.51294] 
Train Loss: 7.6929936
Test Loss: 37.497906


```



### Mse / 2 mini-batch

```
Train Loss:
 [203425.58, 48.540123, 7.430804, 4.2650633, 3.9580827, 3.9136374, 3.9066372, 3.9066372, 3.9066372, 3.9066372, 3.9066372, 3.9066372, 3.9066372, 3.9066372, 3.9066372, 3.9066372]

Test Loss:
 [106578.664, 173.05165, 75.85026, 57.379326, 53.082275, 51.99652, 51.829273, 51.829273, 51.829273, 51.829273, 51.829273, 51.829273, 51.829273, 51.829273, 51.829273, 51.829273]
 
iteration times: 0
W: [[9.718422  ]
 [0.16357741]] 
b: [0.096222] 
Train Loss: 203425.58
Test Loss: 106578.664

iteration times: 100000
W: [[  7.188187]
 [-75.08767 ]] 
b: [52.244297] 
Train Loss: 48.540123
Test Loss: 173.05165

iteration times: 200000
W: [[  6.9165664]
 [-73.08582  ]] 
b: [66.975365] 
Train Loss: 7.430804
Test Loss: 75.85026

iteration times: 300000
W: [[  6.8455234]
 [-72.56188  ]] 
b: [70.82679] 
Train Loss: 4.2650633
Test Loss: 57.379326

iteration times: 400000
W: [[  6.8271694]
 [-72.426956 ]] 
b: [71.82368] 
Train Loss: 3.9580827
Test Loss: 53.082275

iteration times: 500000
W: [[  6.8224297]
 [-72.39154  ]] 
b: [72.07866] 
Train Loss: 3.9136374
Test Loss: 51.99652

iteration times: 600000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 700000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 800000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 900000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 1000000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 1100000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 1200000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 1300000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 1400000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273

iteration times: 1500000
W: [[  6.821652]
 [-72.38649 ]] 
b: [72.12378] 
Train Loss: 3.9066372
Test Loss: 51.829273
```



### Sum 归一化

```
Train Loss:
 [9.5697155, 0.44220608, 0.07687889, 0.013611701, 0.0026181408, 0.0006814125, 0.0003383949, 0.0002806878, 0.0002688958, 0.00026624338, 0.00026582208, 0.00026558578, 0.00026560854, 0.00026557158, 0.0002656439, 0.0002657702]

Test Loss:
 [2.4427087, 0.050131258, 0.005867483, 0.00097511173, 0.0010275528, 0.0014455966, 0.0016744689, 0.0017554929, 0.001824941, 0.001840225, 0.0018675847, 0.0018558321, 0.0018744892, 0.0018714336, 0.0018786299, 0.0018858849]
 
iteration times: 100000
W: [[ 0.44367516]
 [-0.60045993]] 
b: [0.42545658] 
Train Loss: 0.44220608
Test Loss: 0.050131258

iteration times: 200000
W: [[ 0.63352907]
 [-0.8578458 ]] 
b: [0.4516036] 
Train Loss: 0.07687889
Test Loss: 0.005867483

iteration times: 300000
W: [[ 0.7188093]
 [-0.9608501]] 
b: [0.4557397] 
Train Loss: 0.013611701
Test Loss: 0.00097511173

iteration times: 400000
W: [[ 0.7552315]
 [-1.0028976]] 
b: [0.45727262] 
Train Loss: 0.0026181408
Test Loss: 0.0010275528

iteration times: 500000
W: [[ 0.7706382]
 [-1.0204084]] 
b: [0.4579417] 
Train Loss: 0.0006814125
Test Loss: 0.0014455966

iteration times: 600000
W: [[ 0.7771091]
 [-1.0278238]] 
b: [0.4581783] 
Train Loss: 0.0003383949
Test Loss: 0.0016744689

iteration times: 700000
W: [[ 0.779663 ]
 [-1.0307382]] 
b: [0.45819142] 
Train Loss: 0.0002806878
Test Loss: 0.0017554929

iteration times: 800000
W: [[ 0.78081125]
 [-1.0319928 ]] 
b: [0.45830616] 
Train Loss: 0.0002688958
Test Loss: 0.001824941

iteration times: 900000
W: [[ 0.7812952]
 [-1.032626 ]] 
b: [0.45832145] 
Train Loss: 0.00026624338
Test Loss: 0.001840225

iteration times: 1000000
W: [[ 0.781441]
 [-1.03299 ]] 
b: [0.45844987] 
Train Loss: 0.00026582208
Test Loss: 0.0018675847

iteration times: 1100000
W: [[ 0.78163546]
 [-1.0332049 ]] 
b: [0.45837894] 
Train Loss: 0.00026558578
Test Loss: 0.0018558321

iteration times: 1200000
W: [[ 0.78174996]
 [-1.0333698 ]] 
b: [0.4584428] 
Train Loss: 0.00026560854
Test Loss: 0.0018744892

iteration times: 1300000
W: [[ 0.78175914]
 [-1.0333457 ]] 
b: [0.45842263] 
Train Loss: 0.00026557158
Test Loss: 0.0018714336

iteration times: 1400000
W: [[ 0.78176856]
 [-1.0333742 ]] 
b: [0.45845258] 
Train Loss: 0.0002656439
Test Loss: 0.0018786299

iteration times: 1500000
W: [[ 0.78182465]
 [-1.0334235 ]] 
b: [0.45846984] 
Train Loss: 0.0002657702
Test Loss: 0.0018858849
```

