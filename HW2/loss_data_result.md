## Ex1

### Mse / 2 ，归一化

```
Train Loss:
 [0.09566953, 0.010694978, 0.004365819, 0.0018175665, 0.0007625969, 0.00032301687, 0.00013708518, 6.0287868e-05, 2.7964898e-05, 1.2988224e-05, 7.280114e-06, 5.8223914e-06, 5.8223914e-06, 5.8223914e-06, 5.8223914e-06, 5.8223914e-06]

Test Loss:
 [0.122102775, 0.0068427073, 0.0024256264, 0.0008579258, 0.0002942976, 0.000104038816, 5.0109502e-05, 4.431181e-05, 5.1905798e-05, 6.3162064e-05, 7.358234e-05, 7.497685e-05, 7.497685e-05, 7.497685e-05, 7.497685e-05, 7.497685e-05]
 
iteration times: 0
W: [[2.5816895e-05]
 [1.6823726e-05]] 
b: [5.6524175e-05] 
Train Loss: 0.09566953
Test Loss: 0.122102775

iteration times: 100000
W: [[ 0.29399818]
 [-0.34172985]] 
b: [0.38485304] 
Train Loss: 0.010694978
Test Loss: 0.0068427073

iteration times: 200000
W: [[ 0.44556725]
 [-0.60313416]] 
b: [0.42715162] 
Train Loss: 0.004365819
Test Loss: 0.0024256264

iteration times: 300000
W: [[ 0.557119 ]
 [-0.7607374]] 
b: [0.44373533] 
Train Loss: 0.0018175665
Test Loss: 0.0008579258

iteration times: 400000
W: [[ 0.6338206]
 [-0.8587512]] 
b: [0.4508256] 
Train Loss: 0.0007625969
Test Loss: 0.0002942976

iteration times: 500000
W: [[ 0.68474984]
 [-0.92067146]] 
b: [0.4542353] 
Train Loss: 0.00032301687
Test Loss: 0.000104038816

iteration times: 600000
W: [[ 0.71869886]
 [-0.96047455]] 
b: [0.4558635] 
Train Loss: 0.00013708518
Test Loss: 5.0109502e-05

iteration times: 700000
W: [[ 0.7401436 ]
 [-0.98584706]] 
b: [0.45692566] 
Train Loss: 6.0287868e-05
Test Loss: 4.431181e-05

iteration times: 800000
W: [[ 0.75430346]
 [-1.0017053 ]] 
b: [0.45734078] 
Train Loss: 2.7964898e-05
Test Loss: 5.1905798e-05

iteration times: 900000
W: [[ 0.76361966]
 [-1.0136262 ]] 
b: [0.4580757] 
Train Loss: 1.2988224e-05
Test Loss: 6.3162064e-05

iteration times: 1000000
W: [[ 0.7695801]
 [-1.0201465]] 
b: [0.458368] 
Train Loss: 7.280114e-06
Test Loss: 7.358234e-05

iteration times: 1100000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1200000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1300000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1400000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05

iteration times: 1500000
W: [[ 0.7724394]
 [-1.021761 ]] 
b: [0.4578243] 
Train Loss: 5.8223914e-06
Test Loss: 7.497685e-05
```



### Mse / 2 

```
Train Loss:
 [49610.42, 33.503178, 7.535531, 2.9105134, 2.089079, 1.9431273, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444, 1.9135444]

Test Loss:
 [13945.736, 62.241535, 59.77898, 63.006298, 65.11604, 66.11772, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166, 66.6166]
 
iteration times: 100000
W: [[  7.089414]
 [-72.76044 ]] 
b: [46.329998] 
Train Loss: 33.503178
Test Loss: 62.241535

iteration times: 200000
W: [[  6.9002647]
 [-72.54343  ]] 
b: [65.48001] 
Train Loss: 7.535531
Test Loss: 59.77898

iteration times: 300000
W: [[  6.8204446]
 [-72.45184  ]] 
b: [73.5612] 
Train Loss: 2.9105134
Test Loss: 63.006298

iteration times: 400000
W: [[  6.7869325]
 [-72.41338  ]] 
b: [76.95402] 
Train Loss: 2.089079
Test Loss: 65.11604

iteration times: 500000
W: [[  6.773085]
 [-72.3975  ]] 
b: [78.355934] 
Train Loss: 1.9431273
Test Loss: 66.11772

iteration times: 600000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 700000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 800000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 900000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1000000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1100000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1200000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1300000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1400000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166

iteration times: 1500000
W: [[  6.766556]
 [-72.39001 ]] 
b: [79.017] 
Train Loss: 1.9135444
Test Loss: 66.6166
```



### Mse / 2 标准化

```
Train Loss:
 [0.49992374, 5.4192533e-05, 5.3944535e-05, 5.3944543e-05, 5.3944535e-05, 5.3944535e-05, 5.3944543e-05, 5.3944535e-05, 5.3944535e-05, 5.3944543e-05, 5.3944535e-05, 5.3944535e-05, 5.3944543e-05, 5.3944535e-05, 5.3944535e-05, 5.3944543e-05]

Test Loss:
 [0.32022828, 0.0018534504, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731, 0.0018593731]
 
iteration times: 100000
W: [[ 0.9028544]
 [-1.1201917]] 
b: [-1.48920325e-08] 
Train Loss: 5.4192533e-05
Test Loss: 0.0018534504

iteration times: 200000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351838e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 300000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351738e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731

iteration times: 400000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351824e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 500000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351838e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 600000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351738e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731

iteration times: 700000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351824e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 800000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351838e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 900000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351738e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731

iteration times: 1000000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351824e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 1100000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351838e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 1200000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351738e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731

iteration times: 1300000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351824e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 1400000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351838e-08] 
Train Loss: 5.3944535e-05
Test Loss: 0.0018593731

iteration times: 1500000
W: [[ 0.903292 ]
 [-1.1204237]] 
b: [2.2351738e-08] 
Train Loss: 5.3944543e-05
Test Loss: 0.0018593731
```



### Sum 归一化

```
Train Loss:
 [9.29617, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752, 0.00026552752]

Test Loss:
 [2.3778727, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807, 0.0018572807]
 
iteration times: 0
W: [[0.00258169]
 [0.00168237]] 
b: [0.00565242] 
Train Loss: 9.29617
Test Loss: 2.3778727

iteration times: 100000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 200000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 300000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 400000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 500000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 600000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 700000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 800000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 900000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1000000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1100000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1200000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1300000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1400000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807

iteration times: 1500000
W: [[ 0.78165245]
 [-1.033069  ]] 
b: [0.45834905] 
Train Loss: 0.00026552752
Test Loss: 0.0018572807
```





## Ex2

### Mse / 2

```
Train Loss:
 [129877.92, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]

Test Loss:
 [58602.49, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
 
 iteration times: 0
W: [[8.409362 ]
 [0.3310432]] 
b: [0.08022387] 
Train Loss: 129877.92
Test Loss: 58602.49

iteration times: 100000
W: [[nan]
 [nan]] 
b: [nan] 
Train Loss: nan
Test Loss: nan

```



### Mse / 2 归一化

```
Train Loss:
 [0.095660314, 0.007903346, 0.0024309573, 0.0007622265, 0.00024147338, 7.796503e-05, 2.6596446e-05, 1.1197945e-05, 5.463813e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06, 4.4367066e-06]

Test Loss:
 [0.122091904, 0.004828136, 0.0012166699, 0.00029435699, 7.637401e-05, 4.368423e-05, 5.2485626e-05, 6.337282e-05, 7.764882e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05, 7.910379e-05]
 
iteration times: 0
W: [[3.4422526e-05]
 [2.2431634e-05]] 
b: [7.536556e-05] 
Train Loss: 0.095660314
Test Loss: 0.122091904

iteration times: 100000
W: [[ 0.34876078]
 [-0.4445611 ]] 
b: [0.40364408] 
Train Loss: 0.007903346
Test Loss: 0.004828136

iteration times: 200000
W: [[ 0.52424943]
 [-0.71647364]] 
b: [0.43973732] 
Train Loss: 0.0024309573
Test Loss: 0.0012166699

iteration times: 300000
W: [[ 0.63378  ]
 [-0.8588557]] 
b: [0.45082736] 
Train Loss: 0.0007622265
Test Loss: 0.00029435699

iteration times: 400000
W: [[ 0.69782954]
 [-0.93618083]] 
b: [0.45485282] 
Train Loss: 0.00024147338
Test Loss: 7.637401e-05

iteration times: 500000
W: [[ 0.7344782]
 [-0.9788271]] 
b: [0.45655513] 
Train Loss: 7.796503e-05
Test Loss: 4.368423e-05

iteration times: 600000
W: [[ 0.755049 ]
 [-1.0025738]] 
b: [0.4573638] 
Train Loss: 2.6596446e-05
Test Loss: 5.2485626e-05

iteration times: 700000
W: [[ 0.76619464]
 [-1.0145932 ]] 
b: [0.45742148] 
Train Loss: 1.1197945e-05
Test Loss: 6.337282e-05

iteration times: 800000
W: [[ 0.7721551]
 [-1.0231423]] 
b: [0.45845333] 
Train Loss: 5.463813e-06
Test Loss: 7.764882e-05

iteration times: 900000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1000000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1100000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1200000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1300000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1400000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05

iteration times: 1500000
W: [[ 0.7747658]
 [-1.0246164]] 
b: [0.45795682] 
Train Loss: 4.4367066e-06
Test Loss: 7.910379e-05
```



### Mse / 2 标准化

```
Train Loss:
 [0.49989837, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05, 5.3859072e-05]

Test Loss:
 [0.32021007, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311, 0.0018655311]
 
iteration times: 0
W: [[ 6.1949955e-05]
 [-1.2842873e-04]] 
b: [2.9802322e-12] 
Train Loss: 0.49989837
Test Loss: 0.32021007

iteration times: 100000
W: [[ 0.9034343]
 [-1.1205984]] 
b: [-1.5426696e-08] 
Train Loss: 5.3859072e-05
Test Loss: 0.0018655311

...
```



## Ex3

### Mse / 2

```
Train Loss:
 [88862.46, 161.6851, 127.918045, 108.15653, 92.329544, 78.49047, 66.81178, 60.5053, 46.82453, 43.613724, 36.005184, 30.851025, 25.451572, 24.91912, 17.78771, 15.59241]

Test Loss:
 [113332.516, 144.4163, 98.57767, 91.89929, 94.36559, 66.74569, 61.61593, 98.280365, 63.856804, 44.74122, 82.44847, 44.956745, 73.33224, 87.98949, 60.099987, 67.05955]

iteration times: 0
W: [[0.22820099]
 [0.01245753]] 
b: [0.003741] 
Train Loss: 88862.46
Test Loss: 113332.516

iteration times: 100000
W: [[  7.5171194]
 [-73.1933   ]] 
b: [7.3581514] 
Train Loss: 161.6851
Test Loss: 144.4163

iteration times: 200000
W: [[  7.4131837]
 [-73.12359  ]] 
b: [13.298775] 
Train Loss: 127.918045
Test Loss: 98.57767

iteration times: 300000
W: [[  7.365377]
 [-73.12874 ]] 
b: [18.706495] 
Train Loss: 108.15653
Test Loss: 91.89929

iteration times: 400000
W: [[  7.328045]
 [-73.05038 ]] 
b: [23.729918] 
Train Loss: 92.329544
Test Loss: 94.36559

iteration times: 500000
W: [[  7.249613]
 [-72.93348 ]] 
b: [28.344261] 
Train Loss: 78.49047
Test Loss: 66.74569

iteration times: 600000
W: [[  7.209701]
 [-72.94943 ]] 
b: [32.57215] 
Train Loss: 66.81178
Test Loss: 61.61593

iteration times: 700000
W: [[  7.2189336]
 [-72.88338  ]] 
b: [36.447006] 
Train Loss: 60.5053
Test Loss: 98.280365

iteration times: 800000
W: [[  7.1508927]
 [-72.9032   ]] 
b: [40.013485] 
Train Loss: 46.82453
Test Loss: 63.856804

iteration times: 900000
W: [[  7.092128]
 [-72.794174]] 
b: [43.28909] 
Train Loss: 43.613724
Test Loss: 44.74122

iteration times: 1000000
W: [[  7.1114907]
 [-72.77192  ]] 
b: [46.285652] 
Train Loss: 36.005184
Test Loss: 82.44847

iteration times: 1100000
W: [[  7.0416465]
 [-72.72149  ]] 
b: [49.039402] 
Train Loss: 30.851025
Test Loss: 44.956745

iteration times: 1200000
W: [[  7.0510216]
 [-72.677795 ]] 
b: [51.55058] 
Train Loss: 25.451572
Test Loss: 73.33224

iteration times: 1300000
W: [[  7.04244]
 [-72.67177]] 
b: [53.871117] 
Train Loss: 24.91912
Test Loss: 87.98949

iteration times: 1400000
W: [[  6.9947233]
 [-72.63975  ]] 
b: [55.979557] 
Train Loss: 17.78771
Test Loss: 60.099987

iteration times: 1500000
W: [[  6.983691]
 [-72.64849 ]] 
b: [57.92826] 
Train Loss: 15.59241
Test Loss: 67.05955
```



### Mse / 2 归一化

```
Train Loss:
 [0.09569326, 0.026291719, 0.02281258, 0.020685606, 0.018772917, 0.017057486, 0.015512738, 0.014111637, 0.012857355, 0.011722922, 0.010690902, 0.009750936, 0.008906187, 0.008138204, 0.0074382448, 0.0067942357]

Test Loss:
 [0.12213083, 0.02453852, 0.016967352, 0.014727803, 0.013070367, 0.011766525, 0.01053579, 0.009377027, 0.008413457, 0.0076271156, 0.006848436, 0.0061822557, 0.00556877, 0.005002712, 0.004475654, 0.004023999]
 
iteration times: 0
W: [[3.7903471e-06]
 [2.3252617e-06]] 
b: [7.903277e-06] 
Train Loss: 0.09569326
Test Loss: 0.12213083

iteration times: 100000
W: [[0.12153976]
 [0.03973099]] 
b: [0.25352317] 
Train Loss: 0.026291719
Test Loss: 0.02453852

iteration times: 200000
W: [[ 0.1521076 ]
 [-0.00208369]] 
b: [0.29956076] 
Train Loss: 0.02281258
Test Loss: 0.016967352

iteration times: 300000
W: [[ 0.17105478]
 [-0.05198689]] 
b: [0.3168777] 
Train Loss: 0.020685606
Test Loss: 0.014727803

iteration times: 400000
W: [[ 0.18912227]
 [-0.1005742 ]] 
b: [0.33042502] 
Train Loss: 0.018772917
Test Loss: 0.013070367

iteration times: 500000
W: [[ 0.2063749 ]
 [-0.14699244]] 
b: [0.34146872] 
Train Loss: 0.017057486
Test Loss: 0.011766525

iteration times: 600000
W: [[ 0.22443287]
 [-0.19040132]] 
b: [0.35187012] 
Train Loss: 0.015512738
Test Loss: 0.01053579

iteration times: 700000
W: [[ 0.24254754]
 [-0.2314615 ]] 
b: [0.36203918] 
Train Loss: 0.014111637
Test Loss: 0.009377027

iteration times: 800000
W: [[ 0.2599809 ]
 [-0.27020526]] 
b: [0.37066454] 
Train Loss: 0.012857355
Test Loss: 0.008413457

iteration times: 900000
W: [[ 0.2769603 ]
 [-0.30712897]] 
b: [0.3775903] 
Train Loss: 0.011722922
Test Loss: 0.0076271156

iteration times: 1000000
W: [[ 0.29399535]
 [-0.34192628]] 
b: [0.38478577] 
Train Loss: 0.010690902
Test Loss: 0.006848436

iteration times: 1100000
W: [[ 0.31059495]
 [-0.37522328]] 
b: [0.39093933] 
Train Loss: 0.009750936
Test Loss: 0.0061822557

iteration times: 1200000
W: [[ 0.32703787]
 [-0.40623444]] 
b: [0.3966607] 
Train Loss: 0.008906187
Test Loss: 0.00556877

iteration times: 1300000
W: [[ 0.34329104]
 [-0.43549478]] 
b: [0.40206233] 
Train Loss: 0.008138204
Test Loss: 0.005002712

iteration times: 1400000
W: [[ 0.35926524]
 [-0.46319887]] 
b: [0.40733308] 
Train Loss: 0.0074382448
Test Loss: 0.004475654

iteration times: 1500000
W: [[ 0.37492728]
 [-0.48988345]] 
b: [0.41171724] 
Train Loss: 0.0067942357
Test Loss: 0.004023999
```



### Mse / 2 标准化

```
Train Loss:
 [0.4999996, 0.1170655, 0.028649153, 0.007060304, 0.0017479265, 0.00046535794, 0.00015429554, 7.759248e-05, 5.9102196e-05, 5.4844237e-05, 5.4002303e-05, 5.3823944e-05, 5.3769094e-05, 5.3766846e-05, 5.3793578e-05, 5.3804222e-05]

Test Loss:
 [0.32028157, 0.06347892, 0.01259726, 0.0024084095, 0.0009878958, 0.0011555183, 0.0014552241, 0.0016562606, 0.0017807425, 0.0018369947, 0.0018676037, 0.001879819, 0.0018854893, 0.0018891003, 0.001899072, 0.0018974704]
 
iteration times: 0
W: [[-9.000862e-07]
 [-9.989284e-07]] 
b: [2.2309753e-06] 
Train Loss: 0.4999996
Test Loss: 0.32028157

iteration times: 100000
W: [[ 0.415688  ]
 [-0.61216503]] 
b: [0.00163625] 
Train Loss: 0.1170655
Test Loss: 0.06347892

iteration times: 200000
W: [[ 0.65856105]
 [-0.87316483]] 
b: [0.00032234] 
Train Loss: 0.028649153
Test Loss: 0.01259726

iteration times: 300000
W: [[ 0.78210664]
 [-0.9987061 ]] 
b: [-4.5900233e-05] 
Train Loss: 0.007060304
Test Loss: 0.0024084095

iteration times: 400000
W: [[ 0.84370685]
 [-1.0612103 ]] 
b: [0.00017609] 
Train Loss: 0.0017479265
Test Loss: 0.0009878958

iteration times: 500000
W: [[ 0.87424856]
 [-1.0915531 ]] 
b: [9.131243e-06] 
Train Loss: 0.00046535794
Test Loss: 0.0011555183

iteration times: 600000
W: [[ 0.8892055]
 [-1.1065279]] 
b: [-1.592839e-05] 
Train Loss: 0.00015429554
Test Loss: 0.0014552241

iteration times: 700000
W: [[ 0.8967027]
 [-1.1140381]] 
b: [-6.7145855e-05] 
Train Loss: 7.759248e-05
Test Loss: 0.0016562606

iteration times: 800000
W: [[ 0.9003888]
 [-1.1178563]] 
b: [3.8903898e-05] 
Train Loss: 5.9102196e-05
Test Loss: 0.0017807425

iteration times: 900000
W: [[ 0.90226716]
 [-1.1196753 ]] 
b: [7.223291e-07] 
Train Loss: 5.4844237e-05
Test Loss: 0.0018369947

iteration times: 1000000
W: [[ 0.9030391]
 [-1.1205449]] 
b: [1.3940018e-05] 
Train Loss: 5.4002303e-05
Test Loss: 0.0018676037

iteration times: 1100000
W: [[ 0.9034075]
 [-1.1209196]] 
b: [6.174193e-06] 
Train Loss: 5.3823944e-05
Test Loss: 0.001879819

iteration times: 1200000
W: [[ 0.90366083]
 [-1.1211219 ]] 
b: [-6.038601e-06] 
Train Loss: 5.3769094e-05
Test Loss: 0.0018854893

iteration times: 1300000
W: [[ 0.9038631]
 [-1.1213049]] 
b: [-4.7278925e-05] 
Train Loss: 5.3766846e-05
Test Loss: 0.0018891003

iteration times: 1400000
W: [[ 0.9039292]
 [-1.12145  ]] 
b: [2.6012289e-05] 
Train Loss: 5.3793578e-05
Test Loss: 0.001899072

iteration times: 1500000
W: [[ 0.9039004]
 [-1.1214727]] 
b: [-2.1906144e-05] 
Train Loss: 5.3804222e-05
Test Loss: 0.0018974704
```





### Sum 归一化

```
Train Loss:
 [9.5697155, 0.44220608, 0.07687889, 0.013611701, 0.0026181408, 0.0006814125, 0.0003383949, 0.0002806878, 0.0002688958, 0.00026624338, 0.00026582208, 0.00026558578, 0.00026560854, 0.00026557158, 0.0002656439, 0.0002657702]

Test Loss:
 [2.4427087, 0.050131258, 0.005867483, 0.00097511173, 0.0010275528, 0.0014455966, 0.0016744689, 0.0017554929, 0.001824941, 0.001840225, 0.0018675847, 0.0018558321, 0.0018744892, 0.0018714336, 0.0018786299, 0.0018858849]
 
iteration times: 100000
W: [[ 0.44367516]
 [-0.60045993]] 
b: [0.42545658] 
Train Loss: 0.44220608
Test Loss: 0.050131258

iteration times: 200000
W: [[ 0.63352907]
 [-0.8578458 ]] 
b: [0.4516036] 
Train Loss: 0.07687889
Test Loss: 0.005867483

iteration times: 300000
W: [[ 0.7188093]
 [-0.9608501]] 
b: [0.4557397] 
Train Loss: 0.013611701
Test Loss: 0.00097511173

iteration times: 400000
W: [[ 0.7552315]
 [-1.0028976]] 
b: [0.45727262] 
Train Loss: 0.0026181408
Test Loss: 0.0010275528

iteration times: 500000
W: [[ 0.7706382]
 [-1.0204084]] 
b: [0.4579417] 
Train Loss: 0.0006814125
Test Loss: 0.0014455966

iteration times: 600000
W: [[ 0.7771091]
 [-1.0278238]] 
b: [0.4581783] 
Train Loss: 0.0003383949
Test Loss: 0.0016744689

iteration times: 700000
W: [[ 0.779663 ]
 [-1.0307382]] 
b: [0.45819142] 
Train Loss: 0.0002806878
Test Loss: 0.0017554929

iteration times: 800000
W: [[ 0.78081125]
 [-1.0319928 ]] 
b: [0.45830616] 
Train Loss: 0.0002688958
Test Loss: 0.001824941

iteration times: 900000
W: [[ 0.7812952]
 [-1.032626 ]] 
b: [0.45832145] 
Train Loss: 0.00026624338
Test Loss: 0.001840225

iteration times: 1000000
W: [[ 0.781441]
 [-1.03299 ]] 
b: [0.45844987] 
Train Loss: 0.00026582208
Test Loss: 0.0018675847

iteration times: 1100000
W: [[ 0.78163546]
 [-1.0332049 ]] 
b: [0.45837894] 
Train Loss: 0.00026558578
Test Loss: 0.0018558321

iteration times: 1200000
W: [[ 0.78174996]
 [-1.0333698 ]] 
b: [0.4584428] 
Train Loss: 0.00026560854
Test Loss: 0.0018744892

iteration times: 1300000
W: [[ 0.78175914]
 [-1.0333457 ]] 
b: [0.45842263] 
Train Loss: 0.00026557158
Test Loss: 0.0018714336

iteration times: 1400000
W: [[ 0.78176856]
 [-1.0333742 ]] 
b: [0.45845258] 
Train Loss: 0.0002656439
Test Loss: 0.0018786299

iteration times: 1500000
W: [[ 0.78182465]
 [-1.0334235 ]] 
b: [0.45846984] 
Train Loss: 0.0002657702
Test Loss: 0.0018858849
```

